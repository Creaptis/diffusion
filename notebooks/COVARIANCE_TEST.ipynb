{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from utils import timeSteps2stepSize\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# print(\"the device used is\", jax.devices(), jax.default_backend())\n",
    "# print(jax.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "rng = key\n",
    "batch_size = 512\n",
    "intermediate_embeding_time_dimension = 128\n",
    "intermediate_features_embedding = 128\n",
    "# epsilon = 0.01 # smallest time considered\n",
    "num_timesteps = 2000\n",
    "\n",
    "dimension = 2\n",
    "T_max = 1.0\n",
    "\n",
    "num_training_iterations = 100_000\n",
    "\n",
    "print(\"the device used is\", jax.devices(), jax.default_backend())\n",
    "print(jax.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatchPoints(key, distribution = 'plus_sign', batch_size=batch_size) :\n",
    "    if distribution == 'spiral' :\n",
    "        key1, key2 = random.split(key)\n",
    "        radius = random.uniform(key1, shape = (batch_size,1) , minval= 0., maxval= 1.)**.5\n",
    "        noise = 0.01*random.normal(key2, shape = (batch_size,2))\n",
    "        angle = 10*radius # angle from the first axis\n",
    "        positions = jnp.concatenate( (radius*jnp.cos(angle),radius*jnp.sin(angle)), axis=1  ) + noise\n",
    "    \n",
    "    if distribution == 'plus_sign' :\n",
    "        key1, key2 = random.split(key)\n",
    "        radius = random.uniform(key1, shape = (batch_size,1) , minval= -1., maxval= 1.)\n",
    "        axis = random.randint(key2, shape  = (batch_size,1), minval=0, maxval=2)\n",
    "        positions = jnp.concatenate( (radius*axis,radius*(1 - axis)), axis=1  )\n",
    "\n",
    "    return(positions[...,None])\n",
    "\n",
    "class distribution_dataset(Dataset):\n",
    "    def __init__(self, key = random.PRNGKey(seed)) : \n",
    "\n",
    "        self.points = generateBatchPoints(key, distribution = 'spiral', batch_size = batch_size*50 )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]//batch_size \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return( self.points[ idx*batch_size:(idx+1)*batch_size , ...])\n",
    "    \n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                  shuffle=False, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0,\n",
    "                  pin_memory=False, drop_last=False,\n",
    "                  timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng , subkey = random.split(key)\n",
    "trainset = distribution_dataset(subkey)\n",
    "train_dataloader = NumpyLoader(trainset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 6.0\n",
    "net_width = 128 \n",
    "Gamma = 0.\n",
    "\n",
    "M = 0.25 # Magic value from paper \n",
    "nu = 2*jnp.sqrt(M**(-1)) + Gamma # Critically or Non critically damped regime\n",
    "BETA = lambda t : t*beta\n",
    "print(\" values    :   nu \",nu,\"  Gamma \",Gamma,\"  M \",M)\n",
    "if 4*M**(-1) != (Gamma - nu)**2 :\n",
    "    print(\"NON CRITICALLY DAMPED REGIME !!\")\n",
    "else :\n",
    "    print(\"CRITICALLY DAMPED REGIME !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeIndices2RealTime(time_indices ) :\n",
    "    return( time_indices *1.0/num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Sigma in a non-critical general way. We use different notations here.\n",
    "\n",
    "# TODO change those values they are arbitrary \n",
    "Sigma_xx_0 = 0.05\n",
    "Sigma_vv_0 = 0.25\n",
    "\n",
    "\n",
    "# check if we are in the critically damped regime\n",
    "if 4*M**(-1) != (Gamma - nu)**2 :\n",
    "    Sigma_xx = lambda t : (1/(2*M*(-4 + M*(Gamma-nu)**2)))*jnp.exp(-((BETA(t)*(jnp.sqrt(M*(-4 + M*(Gamma-nu)**2)) + M*(Gamma+nu)))/(2*M)))*(2*jnp.exp((BETA(t)*(jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)) + M*(Gamma+ nu)))/(2*M))*M*(-4 + M*(Gamma- nu)**2) + 2*Sigma_vv_0 - 4*jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/(2*M))*(Sigma_vv_0 + M*(-2 + Sigma_xx_0)) + M*(jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)) + M*(Gamma- nu))*(Gamma- nu)*(-1 + Sigma_xx_0) - 2*M*Sigma_xx_0 + jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/ M)*(2*Sigma_vv_0 - M*(Gamma- nu)*(jnp.sqrt( M*(-4 + M*(Gamma- nu)**2)) + M*(-Gamma+ nu))*(-1 + Sigma_xx_0) - 2*M*Sigma_xx_0))\n",
    "    Sigma_xv = lambda t : (1/(2*M*(-4 + M*(Gamma-nu)**2)))*jnp.exp(-((BETA(t)*(jnp.sqrt(M*(-4 + M*(Gamma-nu)**2)) + M*(Gamma+nu)))/(2*M)))*(-jnp.sqrt(M*(-4 + M*(Gamma- nu)**2))*Sigma_vv_0 + jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/M)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2))*Sigma_vv_0 + M*(Gamma- nu)*(Sigma_vv_0 + M*(-2 + Sigma_xx_0)) - 2*jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/(2*M))*M*(Gamma- nu)*(Sigma_vv_0 + M*(-2 + Sigma_xx_0)) + jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/M)*M*(Gamma- nu)*(Sigma_vv_0 + M*(-2 + Sigma_xx_0)) + M*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2))*Sigma_xx_0 -jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/M)*M*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2))*Sigma_xx_0)\n",
    "    Sigma_vv = lambda t : -(1/(2*(-4 + M*(Gamma-nu)**2)))*jnp.exp(-((BETA(t)*(jnp.sqrt(M*(-4 + M*(Gamma-nu)**2)) + M*(Gamma+nu)))/(2*M)))*(-2*jnp.exp((BETA(t)*(jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)) + M*(Gamma+ nu)))/(2*M))*M*(-4 + M*(Gamma- nu)**2) + M**2*(Gamma- nu)**2 + (2 + jnp.sqrt(M*(-4 + M*(Gamma- nu)**2))*(Gamma- nu))*Sigma_vv_0 + 4*jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/(2*M))*(Sigma_vv_0 + M*(-2 + Sigma_xx_0)) - M*((Gamma- nu)*(jnp.sqrt( M*(-4 + M*(Gamma- nu)**2)) + Gamma*Sigma_vv_0 - nu*Sigma_vv_0) + 2*Sigma_xx_0) + jnp.exp((BETA(t)*jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)))/M)*(M**2*(Gamma- nu)**2 + 2*Sigma_vv_0 + jnp.sqrt(M*(-4 + M*(Gamma- nu)**2))*(-Gamma+ nu)*Sigma_vv_0 + M*(Gamma- nu)*(jnp.sqrt(M*(-4 + M*(Gamma- nu)**2)) - Gamma*Sigma_vv_0 + nu*Sigma_vv_0) - 2*M*Sigma_xx_0))\n",
    "\n",
    "    def mu_global_HSM(t_batch, x_0_batch, v_0_batch = None) :\n",
    "        \"\"\" \n",
    "        input :\n",
    "        - t_batch : (batch_size,)\n",
    "        - x_0_batch : (batch_size,dimension,1) or (batch_size,dimension)\n",
    "        - v_0_batch (optional) : (batch_size,dimension,1) or (batch_size,dimension)\n",
    "        output :\n",
    "        - mu : (batch_size,2,dimension)\n",
    "        \"\"\"\n",
    "        if v_0_batch is None :\n",
    "            v_0_batch = jnp.zeros(x_0_batch.shape)\n",
    "        \n",
    "        x0 = x_0_batch.reshape(batch_size, dimension)\n",
    "        v0 = v_0_batch.reshape(batch_size, dimension)\n",
    "        t = t_batch.reshape((-1,1))\n",
    "        print(\"ee\",BETA(t).shape)\n",
    "\n",
    "        mu_v = (jnp.exp(-((BETA(t)*(jnp.sqrt(-4 + M*(Gamma - nu)**2) + jnp.sqrt(M)*(Gamma + nu)))/( 4*jnp.sqrt(M))))*((1 + jnp.exp(( BETA(t)*jnp.sqrt(-4 + M*(Gamma - nu)**2))/(2*jnp.sqrt(M))))*v0*jnp.sqrt(-4 + M*(Gamma - nu)**2) - (-1 + jnp.exp(( BETA(t)*jnp.sqrt(-4 + M*(Gamma - nu)**2))/( 2*jnp.sqrt(M))))*jnp.sqrt(M)*(2*x0 + v0*(-Gamma + nu))))/( 2*jnp.sqrt(-4 + M*(Gamma - nu)**2) )\n",
    "        mu_x = (jnp.exp(-((BETA(t)*(jnp.sqrt(-4 + M*(Gamma - nu)**2) + jnp.sqrt(M)*(Gamma + nu)))/( 4*jnp.sqrt(M))))*(2*(-1 + jnp.exp(( BETA(t)*jnp.sqrt(-4 + M*(Gamma - nu)**2))/( 2*jnp.sqrt(M))))*v0 + (1 + jnp.exp(( BETA(t)*jnp.sqrt(-4 + M*(Gamma - nu)**2))/(2*jnp.sqrt(M))))*jnp.sqrt(M)*x0*jnp.sqrt(-4 + M*(Gamma - nu)**2) - (-1 + jnp.exp(( BETA(t)*jnp.sqrt(-4 + M*(Gamma - nu)**2))/( 2*jnp.sqrt(M))))*M*x0*(Gamma - nu)))/( 2*jnp.sqrt(M)*jnp.sqrt(-4 + M*(Gamma - nu)**2))\n",
    "        print(mu_v.shape)\n",
    "        print(mu_x.shape)\n",
    "        \n",
    "        # (2, batch_size ,  dim) -> (batch_size, 2, dim)\n",
    "        return jnp.array([mu_x, mu_v ]).transpose( (1,0,2) )\n",
    "else :\n",
    "\n",
    "    A1 = 1./(4*M)\n",
    "    A2 = M**(-2)/8.\n",
    "    A2 = M**(-2)/4. # TODO REVOIR !!\n",
    "    A3 = (nu-Gamma)/2.\n",
    "    A4 = -M**(-1)/2. \n",
    "    A5 = (Gamma-nu)/2.\n",
    "    C1 = (Gamma-nu)/8.\n",
    "    C2 = (Gamma-nu)**3/32.\n",
    "    C3 = -1/2.\n",
    "    C4 = M**(-1)/2.\n",
    "    C5 = (nu-Gamma)/4.\n",
    "    D1 = 1/4.\n",
    "    D2 = M**(-1)/4.\n",
    "    D3 = (Gamma-nu)/2.\n",
    "    D4 = -1/2.\n",
    "    D5 = M*(nu-Gamma)/4.\n",
    "    Sigma_xx = lambda t : jnp.exp(-(Gamma+nu)/2.*BETA(t))*( A1*BETA(t)**2*Sigma_xx_0 + A2*BETA(t)**2*Sigma_vv_0 + A3*BETA(t)*Sigma_xx_0 + A4*BETA(t)**2 + A5*BETA(t) + (jnp.exp((Gamma + nu)/2.*BETA(t)) - 1) + Sigma_xx_0)\n",
    "    Sigma_xv = lambda t : jnp.exp(-(Gamma+nu)/2.*BETA(t))*( C1*BETA(t)**2*Sigma_xx_0 + C2*BETA(t)**2*Sigma_vv_0 + C3*BETA(t)*Sigma_xx_0 + C4*BETA(t)*Sigma_vv_0 + C5*BETA(t)**2)\n",
    "    Sigma_vv = lambda t : jnp.exp(-(Gamma+nu)/2.*BETA(t))*( D1*BETA(t)**2*Sigma_xx_0 + D2*BETA(t)**2*Sigma_vv_0 + D3*BETA(t)*Sigma_vv_0 + D4*BETA(t)**2 + D5*BETA(t) + M*(jnp.exp((Gamma + nu)/2.*BETA(t)) - 1) + Sigma_vv_0)\n",
    "\n",
    "    # Sigma_xx = lambda t : 1+1/8*jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*(-8-BETA(t)*(-4+BETA(t)*(Gamma-nu))*(Gamma-nu))+1/64*jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*BETA(t)**2*(Gamma-nu)**4*Sigma_vv_0+1/16*jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*(-4+BETA(t)*(Gamma-nu))**2* Sigma_xx_0\n",
    "    # Sigma_xv = lambda t : 1/4*jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*BETA(t)**2*(-Gamma+nu)+1/32*jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*BETA(t)*(4+BETA(t)*(Gamma-nu))*(Gamma-nu)**2* Sigma_vv_0+1/8*jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*BETA(t)*(-4+BETA(t)*(Gamma-nu))*Sigma_xx_0\n",
    "    # Sigma_vv = lambda t : (jnp.exp(-(1/2)*BETA(t)*(Gamma+nu))*(-8+8*jnp.exp(1/2*BETA(t)*(Gamma+nu))-BETA(t)* (4+BETA(t)*(Gamma-nu))*(Gamma-nu)))/(2*(Gamma-nu)**2)+1/16*jnp.exp(-(1/2)*BETA(t)* (Gamma+nu))*(4+BETA(t)*(Gamma-nu))**2*Sigma_vv_0+1/4*jnp.exp(-(1/2)*BETA(t)* (Gamma+nu))*BETA(t)**2*Sigma_xx_0\n",
    "\n",
    "    def mu_global_HSM(t_batch, x_0_batch, v_0_batch = None) :\n",
    "        \"\"\" \n",
    "        input :\n",
    "        - t_batch : (batch_size,)\n",
    "        - x_0_batch : (batch_size,dimension,1) or (batch_size,dimension)\n",
    "        - v_0_batch (optional) : (batch_size,dimension,1) or (batch_size,dimension)\n",
    "        output :\n",
    "        - mu : (batch_size,2,dimension)\n",
    "        \"\"\"\n",
    "        if v_0_batch is None :\n",
    "            v_0_batch = jnp.zeros(x_0_batch.shape)\n",
    "        \n",
    "        x0 = x_0_batch.reshape(batch_size, dimension)\n",
    "        v0 = v_0_batch.reshape(batch_size, dimension)\n",
    "        t = t_batch.reshape((-1,1))\n",
    "        print(\"ee\",BETA(t).shape)\n",
    "\n",
    "        mu_v = jnp.exp(-(BETA(t)*(Gamma + nu)/4.0))*(  (nu-Gamma)/4.*BETA(t)*x0 + (nu-Gamma)**2/8.*BETA(t)*v0 + x0  )\n",
    "        mu_x = jnp.exp(-(BETA(t)*(Gamma + nu)/4.0))*(  -1/2.*BETA(t)*x0 + (Gamma-nu)/4.*BETA(t)*v0 + v0 )\n",
    "        print(mu_v.shape)\n",
    "        print(mu_x.shape)\n",
    "        \n",
    "        # (2, batch_size ,  dim) -> (batch_size, 2, dim)\n",
    "        return jnp.array([mu_x, mu_v ]).transpose( (1,0,2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : does the forward give the same covariance as predicted by our formulas \n",
    "experimental_batch_size = 10000\n",
    "\n",
    "def forward_step(positions, stepSize, key) :\n",
    "    \"\"\" \n",
    "    positions : (experimental_batch_size,2,2)\n",
    "    stepSize : (experimental_batch_size)\n",
    "    \"\"\" \n",
    "    positions = positions.reshape((experimental_batch_size,2,2))\n",
    "    # TODO : accomodate for when f and G depend on time through beta\n",
    "    f = jnp.array([[-Gamma,M**(-1) ], [ -1, -nu]])*beta/2\n",
    "    G = jnp.array([[jnp.sqrt(Gamma*beta),0 ], [ 0 , jnp.sqrt(M*nu*beta)]])\n",
    "    positions = positions + f@positions*stepSize[:,None,None] + G@(jnp.sqrt(stepSize[:,None,None])*random.normal(key, shape=positions.shape))\n",
    "    return positions.reshape((experimental_batch_size,2,2))\n",
    "\n",
    "def get_theoretical_covariance( i ) :\n",
    "    time_indices = i\n",
    "    time = timeIndices2RealTime( time_indices )\n",
    "    \n",
    "    return jnp.array([[Sigma_xx(time),Sigma_xv(time)],[Sigma_xv(time) , Sigma_vv(time)]])\n",
    "\n",
    "def get_experimental_covariance(positions) :\n",
    "    \"\"\" \n",
    "    positions : shape (experimental_batch_size,2,dim)\n",
    "    \"\"\" \n",
    "    positions = positions.transpose((0,2,1))\n",
    "    positions = positions - jnp.mean(positions, axis = 0)[None,...]\n",
    "    \n",
    "    return jnp.mean( positions[...,None]@positions[...,None,:] , axis = 0 )\n",
    "\n",
    "\n",
    "def forward_covariances(init_position, key) :\n",
    "    \"\"\" \n",
    "    intput :\n",
    "    - init_position : shape (2)\n",
    "    \"\"\" \n",
    "    key, subkey1, subkey2 = random.split(key,3)\n",
    "    experimental_covariance_array = jnp.zeros(shape= (num_timesteps, 2, 2, 2))\n",
    "    theoretical_covariance_array = jnp.zeros(shape= (num_timesteps,2, 2))\n",
    "    positions = jnp.zeros(shape = (num_timesteps, experimental_batch_size, 2,2) )\n",
    "    positions = positions.at[0,:,0,:].set(init_position[None,:])\n",
    "\n",
    "    # noise the input according to Sigma_0\n",
    "    positions = positions.at[0,:,0,:].set(positions[0,:,0,:] + jnp.sqrt(Sigma_xx_0)*random.normal(subkey1, shape= positions[0,:,0,:].shape))\n",
    "    positions = positions.at[0,:,1,:].set(positions[0,:,1,:] + jnp.sqrt(Sigma_vv_0)*random.normal(subkey2, shape= positions[0,:,1,:].shape))\n",
    "\n",
    "    stepSize = timeSteps2stepSize(num_timesteps, experimental_batch_size)\n",
    "\n",
    "    print('positions shape', positions.shape)\n",
    "    for i in range(num_timesteps) :\n",
    "        theoretical_covariance_array = theoretical_covariance_array.at[i].set(get_theoretical_covariance(i))\n",
    "        experimental_covariance_array = experimental_covariance_array.at[i].set(get_experimental_covariance(positions[i]))\n",
    "\n",
    "        positions = positions.at[i+1].set( forward_step(positions[i],stepSize[i], key))\n",
    "        key, = random.split(key,1)\n",
    "\n",
    "    plot_covariances_evolution(theoretical_covariance_array, experimental_covariance_array)\n",
    "\n",
    "def plot_covariances_evolution(theoretical_covariance_array, experimental_covariance_array) :\n",
    "    timesteps = list(range(num_timesteps))\n",
    "    for k in range(2):\n",
    "        for j in range(2):\n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.plot(timesteps, experimental_covariance_array[:,0,k,j], label='experimental1')\n",
    "            plt.plot(timesteps, experimental_covariance_array[:,1,k,j], label='experimental2')\n",
    "            plt.plot(timesteps, theoretical_covariance_array[:,k,j], label='theoretical')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
